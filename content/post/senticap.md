---
title: "SentiCap: Generating Image Descriptions with Sentiments"
description: "A technique and dataset for generating image captions with strong positive or negative sentiment."
date: "2016-01-03"
draft: false
categories:
  - "research"
  - "paper"
  - "data"
  - "code"
tags:
  - "deeplearning"
  - "vision"
  - "language"
---

##### posted by _Alex Mathews_ <br />

<img src="/img/senticap/intro_example2.png" width="800"><br>

The recent progress on image recognition and language modeling is making automatic description of image content a reality. However, stylized, non-factual aspects of the written description are missing from the current systems. 

<!--more-->

One such style is descriptions with emotions, which is commonplace in everyday communication, and influences decision-making and interpersonal relationships. We design a system to describe an image with emotions, and present a model that automatically generates captions with positive or negative sentiments. We propose a novel switching recurrent neural network with word-level regularization, which is able to produce emotional image captions using only 2000+ training sentences containing sentiments. We evaluate the captions with different automatic and crowd-sourcing metrics. Our model compares favourably in common quality metrics for image captioning. In 84.6% of cases the generated positive captions were judged as being at least as descriptive as the factual captions. Of these positive captions 88% were confirmed by the crowd-sourced workers as having the appropriate sentiment.

Sample Results
--------------------

<img src="/img/senticap/3x4.jpg" width="500"><br>
Examples of captions generated by SentiCap. The captions in columns <b>a</b> and <b>b</b> express a positive sentiment, while the captions in columns <b>c</b> and <b>d</b> express a negative sentiment. The coloring behind words indicates the weight given to the model trained on the sentiment dataset. The darker the coloring the higher the weight. See the paper for full details.


Resources
--------------------

The paper is published in AAAI 2016
_SentiCap: Generating Image Descriptions with Sentiments_, by Alex Mathews, Lexing Xie, Xuming He. [http://arxiv.org/abs/1510.01431](http://arxiv.org/abs/1510.01431)

<!-- Supplemental can be found here: <a href="http://users.cecs.anu.edu.au/~u4534172/papers/senticap_suppliment.pdf">pdf</a><br>
-->

* A combined PDF of the paper and supplemental material is [here](https://arxiv.org/pdf/1510.01431.pdf).
* Example results: sentences with [positive sentiment](http://users.cecs.anu.edu.au/~u4534172/senticap_results_pos.html) and [negative sentiment](http://users.cecs.anu.edu.au/~u4534172/senticap_results_neg.html). 
* The SentiCap dataset collected from Amazon mTurk is [here](http://users.cecs.anu.edu.au/~u4534172/data/Senticap/senticap_dataset.zip). 
* The list of Adjective Noun Pairs (ANPs) is <a href="http://users.cecs.anu.edu.au/~u4534172/data/Senticap/anp_list.txt">here</a>
* The original code is [here](https://users.cecs.anu.edu.au/~u4534172/data/senticap_code.zip).  It uses theano without neural network libraries (because at the time this was the best option).
* All training/test data and pre-trained models are made available [here](https://users.cecs.anu.edu.au/~u4534172/data/senticap_data_models.zip). 
To use, one will need to unzip both into the same directory and then follow the README to train/test the model. CNN fc7 features are read from disk -- they are included with the data -- so to use a new set of images, please extract features using your favorite CNN (code to do this is not included).